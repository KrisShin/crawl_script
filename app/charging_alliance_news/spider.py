import asyncio
import json
import random
import re
import time
from bs4 import BeautifulSoup
import httpx
from loguru import logger
from app.charging_alliance_news.model import ChargingAllianceNews
from app.common.hunyuan_api import call_hunyuan
from common.global_variant import config

URL_PARAMS = {
    "sub": "list",
    "search_field": None,
    "begin": 0,
    "count": "5",
    "query": "",
    "fakeid": config.charging_alliance.fakeid,
    "type": "101_1",
    "free_publish_type": "1",
    "sub_action": "list_ex",
    "fingerprint": config.charging_alliance.fingerprint,
    "token": config.charging_alliance.token,
    "lang": "zh_CN",
    "f": "json",
    "ajax": "1",
}

HEADERS = {
    'Cookie': config.charging_alliance.COOKIE,
}

LLM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè§£æâ€œç”µåŠ¨æ±½è½¦å……ç”µåŸºç¡€è®¾æ–½è¿è¡Œæƒ…å†µâ€æ–°é—»çš„**JSONè½¬æ¢å¼•æ“**ã€‚
ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯å°†éç»“æ„åŒ–æ–‡æœ¬è½¬æ¢ä¸º**ä¸¥æ ¼ç¬¦åˆSchema**çš„JSONæ•°æ®ã€‚

### ğŸš¨ æœ€é«˜ä¼˜å…ˆçº§ç¦ä»¤ (è¿åå³å¤±è´¥)
1.  **ç¦æ­¢Markdown**ï¼šè¾“å‡º**å¿…é¡»**ä»¥ `{` å¼€å¤´ï¼Œä»¥ `}` ç»“å°¾ã€‚ä¸¥ç¦åŒ…å« ```json æˆ– ``` æ ‡è®°ã€‚
2.  **ç¦æ­¢å•ä½**ï¼šæ‰€æœ‰æ•°å€¼å¿…é¡»æ˜¯**çº¯æ•°å­—**ï¼ˆFloatï¼‰ã€‚
    - âŒ é”™è¯¯ï¼š`"1281.8ä¸‡"`, `"59.1äº¿"`, `"31.3%"`
    - âœ… æ­£ç¡®ï¼š`1281.8`, `59.1`, `31.3`
3.  **ç¦æ­¢å¤šä½™å­—æ®µ**ï¼š**åªå…è®¸**è¾“å‡ºâ€œå¾…æå–å­—æ®µåˆ—è¡¨â€ä¸­å®šä¹‰çš„ Keyã€‚ä¸¥ç¦è‡ªä½œèªæ˜æ·»åŠ  `top10_regions`, `region_data` ç­‰å­—æ®µã€‚
4.  **ç¦æ­¢å¢é‡æ··æ·†**ï¼šç»ä¸è¦æŠŠâ€œå¢é‡/å¢åŠ â€çš„æ•°æ®å¡«å…¥â€œæ€»é‡/ä¿æœ‰é‡â€å­—æ®µã€‚

### å­—æ®µæå–é€»è¾‘

**1. æ—¶é—´å®šä½ (Year/Month)**
   - **Year**: ä¼˜å…ˆä»æ ‡é¢˜æå–ã€‚
   - **Month**:
     - ä¼˜å…ˆä»æ ‡é¢˜æå–ï¼ˆå¦‚â€œ2025å¹´4æœˆ...â€ -> 4ï¼‰ã€‚
     - **ç‰¹æ®Šæƒ…å†µ**ï¼šå¦‚æœæ ‡é¢˜åªæœ‰å¹´ä»½ï¼ˆå¦‚â€œ2024å¹´å…¨å›½...â€ï¼‰ï¼Œè¯·é˜…è¯»æ­£æ–‡ **â€œ1 å…¬å…±å……ç”µåŸºç¡€è®¾æ–½è¿è¡Œæƒ…å†µâ€** çš„ç¬¬ä¸€å¥è¯ã€‚
     - *ç¤ºä¾‹*ï¼šâ€œ2024å¹´12æœˆæ¯”...â€ -> åˆ™æœˆä»½ä¸º 12ã€‚

**2. å…³é”®æ•°å€¼æå– (æ ¸å¿ƒè§„åˆ™)**
   - **`public_charging_facilities` (å…¬å…±ä¿æœ‰é‡)**
     - ç›®æ ‡ï¼šæˆªè‡³å½“å‰æ—¶é—´çš„**ç´¯è®¡æ€»æ•°**ã€‚
     - å…³é”®è¯é”šç‚¹ï¼šâ€œæˆªè‡³...å…¬å…±å……ç”µæ¡©...ä¸‡å°/ä¸‡ä¸ªâ€ã€‚
     - *æ’é™¤*ï¼šä¸è¦æå–â€œå¢åŠ â€ã€â€œæ–°å¢â€çš„æ•°å­—ã€‚
   
   - **`private_charging_facilities` (ç§äººä¿æœ‰é‡)**
     - ç›®æ ‡ï¼šæˆªè‡³å½“å‰æ—¶é—´çš„**ç´¯è®¡æ€»æ•°**ã€‚
     - *é™·é˜±è­¦ç¤º*ï¼šå¾ˆå¤šæ–‡ç« åªæåˆ°â€œéšè½¦é…å»ºç§äººå……ç”µæ¡©**å¢é‡**ä¸º...â€ã€‚å¦‚æœä½ åªæ‰¾åˆ°äº†â€œå¢é‡â€ï¼Œ**è¯·å°†ä¿æœ‰é‡å­—æ®µå¡« null**ï¼Œä¸è¦æŠŠå¢é‡å¡«è¿›å»ï¼

   - **`year_NEV_sales` (æ–°èƒ½æºæ±½è½¦å¹´åº¦ç´¯è®¡é”€é‡)**
     - ç›®æ ‡ï¼šæœ¬å¹´åº¦ï¼ˆ1-Xæœˆï¼‰çš„**ç´¯è®¡é”€é‡**ã€‚
     - å…³é”®è¯é”šç‚¹ï¼šæ–‡ç« æœ«å°¾â€œå……ç”µåŸºç¡€è®¾æ–½ä¸ç”µåŠ¨æ±½è½¦å¯¹æ¯”æƒ…å†µâ€ç« èŠ‚ã€‚
     - åŒ¹é…é€»è¾‘ï¼šæ‰¾ â€œ1-Xæœˆ...æ–°èƒ½æºæ±½è½¦é”€é‡...ä¸‡è¾†â€ã€‚å³ä¾¿åŸæ–‡è¯´æ˜¯â€œ1-5æœˆâ€ï¼Œä¹Ÿæå–è¯¥æ•°å­—ä½œä¸ºå¹´åº¦ç´¯è®¡å€¼ã€‚

**3. å¢é‡å­—æ®µ (Increase)**
   - ä»…æå–æ˜ç¡®å¸¦æœ‰â€œå¢åŠ â€ã€â€œæ–°å¢â€ã€â€œå¢é‡â€æè¿°çš„æ•°å­—ã€‚

### å¾…æå–å­—æ®µåˆ—è¡¨ (JSON Schema)
è¯·ä¸¥æ ¼ä»…è¿”å›åŒ…å«ä»¥ä¸‹ Key çš„ JSON å¯¹è±¡ï¼ˆæœªæ‰¾åˆ°å¡« nullï¼‰ï¼š

{
    "total_charging_facilities": null,    // (float) åŸºç¡€è®¾æ–½ç´¯è®¡æ•°é‡ (ä¸‡å°/ä¸‡ä¸ª)
    "public_charging_facilities": null,   // (float) å…¬å…±æ¡©ç´¯è®¡æ•°é‡ (ä¸‡å°/ä¸‡ä¸ª)
    "private_charging_facilities": null,  // (float) ç§äººæ¡©ç´¯è®¡æ•°é‡ (ä¸‡å°/ä¸‡ä¸ª) [æ³¨æ„ï¼šæ‰¾ä¸åˆ°ç´¯è®¡å€¼å¡«nullï¼Œåˆ«å¡«å¢é‡]
    "public_rated_total_power": null,     // (float) å…¬å…±æ¡©é¢å®šæ€»åŠŸç‡ (äº¿åƒç“¦)
    "public_average_power": null,         // (float) å…¬å…±æ¡©å¹³å‡åŠŸç‡ (åƒç“¦)
    "private_declared_capacity": null,    // (float) ç§äººæ¡©æŠ¥è£…å®¹é‡ (äº¿åƒä¼å®‰)
    "total_charging_capacity": null,      // (float) å…¨å›½å……ç”µæ€»ç”µé‡ (äº¿åº¦/äº¿kWh)
    "increase_charging_facilities": null, // (float) [å¢é‡] åŸºç¡€è®¾æ–½å¢é‡
    "increase_public_facilities": null,   // (float) [å¢é‡] å…¬å…±æ¡©å¢é‡
    "increase_private_facilities": null,  // (float) [å¢é‡] ç§äººæ¡©å¢é‡
    "year_NEV_sales": null                // (float) æœ¬å¹´åº¦/1-Xæœˆç´¯è®¡é”€é‡ (ä¸‡è¾†)
}
"""


def extract_article_text(html_content):
    """
    ä»å¾®ä¿¡æ–‡ç«  HTML ä¸­æå–æ­£æ–‡æ–‡æœ¬
    """
    try:
        soup = BeautifulSoup(html_content, 'lxml')

        # 1. å®šä½æ­£æ–‡å®¹å™¨
        # å¾®ä¿¡æ–‡ç« çš„æ­£æ–‡é€šå¸¸åœ¨ id="js_content" çš„ div ä¸­
        content_div = soup.find('div', id='js_content')

        if not content_div:
            logger.warning("æœªæ‰¾åˆ° id='js_content' çš„æ­£æ–‡å®¹å™¨")
            return ""

        # 2. ç§»é™¤æ— ç”¨çš„æ ‡ç­¾ (å¯é€‰)
        # æ¯”å¦‚ script, style æ ‡ç­¾ï¼Œè™½ç„¶ get_text é€šå¸¸ä¼šå¿½ç•¥å®ƒä»¬ï¼Œä½†æ˜¾å¼ç§»é™¤æ›´å®‰å…¨
        for script in content_div(["script", "style"]):
            script.extract()

        # 3. æå–æ–‡æœ¬
        # separator='\n' ä¿è¯æ®µè½ä¹‹é—´æœ‰æ¢è¡Œ
        # strip=True å»é™¤é¦–å°¾ç©ºæ ¼
        lines = []
        for text in content_div.stripped_strings:
            # è¿‡æ»¤æ‰ä¸€äº›å¯èƒ½æ˜¯å¸ƒå±€äº§ç”Ÿçš„æçŸ­æ— æ„ä¹‰å­—ç¬¦ï¼Œæˆ–è€…ä¿ç•™æ‰€æœ‰
            if text.strip():
                lines.append(text.strip())

        # 4. æ‹¼æ¥ç»“æœ
        full_text = '\n'.join(lines)
        return full_text

    except Exception as e:
        logger.error(f"è§£æ HTML å‡ºé”™: {e}")
        return ""


async def parse_page(title: str, article_url: str):
    """
    è¯·æ±‚æ–‡ç« è¯¦æƒ…é¡µï¼Œè§£ææ–‡æœ¬ï¼Œå¹¶è°ƒç”¨å¤§æ¨¡å‹æå–æ•°æ®
    """
    try:
        logger.info(f"æ­£åœ¨æŠ“å–æ–‡ç« : {article_url}")
        response = httpx.get(
            article_url,
            headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 Edg/142.0.0.0'
            },
            timeout=10,
        )

        if response.status_code == 200:
            # 1. æå–çº¯æ–‡æœ¬
            text_content = extract_article_text(response.text)
            if not text_content:
                logger.warning("æœªèƒ½æå–åˆ°æ­£æ–‡å†…å®¹")
                return

            # 2. è°ƒç”¨æ··å…ƒå¤§æ¨¡å‹æå–æ•°æ®
            logger.info("æ­£åœ¨è°ƒç”¨æ··å…ƒæ¨¡å‹æå–æ•°æ®...")
            try:
                # è°ƒç”¨ LLM
                resp_json = call_hunyuan(text_content, LLM_PROMPT)

                # 3. ä¿å­˜æ•°æ®
                # åˆ›å»ºæ•°æ®å¯¹è±¡
                defaults_data = {
                    **resp_json,
                    'title': title,
                    'link': article_url,
                    'origin_text': text_content,
                    'digest': text_content[:200] if text_content else "",
                }

                # get_or_create è¿”å›çš„æ˜¯ä¸€ä¸ªå…ƒç»„ (å¯¹è±¡, æ˜¯å¦åˆ›å»º)
                news_data, created = await ChargingAllianceNews.get_or_create(
                    year=resp_json['year'],  # æŸ¥è¯¢æ¡ä»¶ 1
                    month=resp_json['month'],  # æŸ¥è¯¢æ¡ä»¶ 2
                    defaults=defaults_data,  # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œåˆ›å»ºæ–°å¯¹è±¡æ—¶ä½¿ç”¨çš„é»˜è®¤å€¼
                )

                if created:
                    logger.success(f"æ–°å¢æ•°æ®: {news_data.year}å¹´{news_data.month}æœˆ")
                else:
                    logger.info(f"æ•°æ®å·²å­˜åœ¨: {news_data.year}å¹´{news_data.month}æœˆ")

                # å¡«å…… LLM æå–çš„æ•°æ®
                # éå† JSON é”®å€¼å¯¹å¹¶è®¾ç½®åˆ°æ¨¡å‹ä¸­
                for key, value in resp_json.items():
                    # ç¡®ä¿ key å­˜åœ¨äºæ¨¡å‹å­—æ®µä¸­ï¼Œé˜²æ­¢ LLM å¹»è§‰é€ å‡ºä¸å­˜åœ¨çš„å­—æ®µæŠ¥é”™
                    if hasattr(news_data, key):
                        setattr(news_data, key, value)
                    else:
                        logger.debug(f"å¿½ç•¥æ¨¡å‹ä¸­ä¸å­˜åœ¨çš„å­—æ®µ: {key}")

                # ç‰¹æ®Šé€»è¾‘ï¼šè®¡ç®— NEV_sales (å½“æœˆé”€é‡)
                # å¦‚æœ LLM æ²¡æœ‰æå–åˆ°å½“æœˆé”€é‡ï¼ˆå› ä¸ºæ–‡ä¸­å¯èƒ½åªæœ‰ç´¯è®¡ï¼‰ï¼Œ
                # ä½†ä½ æœ‰ä¸Šä¸ªæœˆçš„ç´¯è®¡é”€é‡æ•°æ®ï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œè¿›è¡ŒäºŒæ¬¡è®¡ç®—ã€‚
                # if news_data.NEV_sales is None and news_data.year_NEV_sales:
                #     last_month_data = await ChargingAllianceNews.get_or_none(year=..., month=...)
                #     if last_month_data:
                #          news_data.NEV_sales = news_data.year_NEV_sales - last_month_data.year_NEV_sales

                # ä¿å­˜åˆ°æ•°æ®åº“
                await news_data.save()
                logger.success(f"æ•°æ®æå–å¹¶ä¿å­˜æˆåŠŸ! å¹´ä»½: {news_data.year}, æœˆä»½: {news_data.month}")

            except Exception as e:
                logger.error(f"å¤§æ¨¡å‹æå–æˆ–ä¿å­˜æ•°æ®å¤±è´¥: {e}")

        else:
            logger.error(f"è¯·æ±‚æ–‡ç« å¤±è´¥, çŠ¶æ€ç : {response.status_code}")

    except Exception as e:
        logger.error(f"æŠ“å–æ–‡ç« å‘ç”Ÿå¼‚å¸¸: {e}")


async def parse_list(begin: int, client: httpx.AsyncClient):
    while True:
        logger.info(f'start crawling begin: {begin}')
        params = URL_PARAMS
        params['begin'] = begin
        response = await client.get(config.charging_alliance.URL, params=URL_PARAMS, headers=HEADERS, timeout=None)
        time.sleep(random.randint(1, 3))
        if response.status_code != 200:
            time.sleep(3600)
            continue
        data = response.json()
        if data['base_resp']['ret'] == 200013:
            # æµé‡æ§åˆ¶, åœæ­¢ä¸€å°æ—¶åå°è¯•
            logger.warning('æµé‡æ§åˆ¶, åœæ­¢ä¸€å°æ—¶åå°è¯•')
            time.sleep(3600)
            continue
        elif data['base_resp']['ret'] == 200003:
            # æ²¡æœ‰Cookieæˆ–è€…Cookieè¿‡æœŸ, ç»ˆæ­¢å°è¯•
            raise Exception('Cookieè¿‡æœŸ')
        elif data['base_resp']['ret'] != 0:
            # æœªçŸ¥é”™è¯¯, ç»ˆæ­¢å°è¯•
            raise Exception('Cookieè¿‡æœŸ')
        publish_page = json.loads(data['publish_page'])
        if not publish_page:
            logger.info(f'çˆ¬å–å·²å®Œæˆ, å…±{begin}æ¡æ•°æ®')
            return
        for pl in publish_page['publish_list']:
            pi = json.loads(pl['publish_info'])
            for news in pi['appmsgex']:
                if news['title'].startswith("ä¿¡æ¯å‘å¸ƒ") and news['title'].endswith("å…¨å›½ç”µåŠ¨æ±½è½¦å……æ¢ç”µåŸºç¡€è®¾æ–½è¿è¡Œæƒ…å†µ"):
                    if await ChargingAllianceNews.filter(year=news['year'], month=news['month']).exists():
                        logger.warning('ä¹‹å‰æ•°æ®å·²çˆ¬å–, ç»“æŸçˆ¬è™«')
                        await parse_page(news['title'], news['link'])

        begin += 5
        time.sleep(random.randint(10, 30) / 1)


async def main():
    begin = 0
    logger.info(f'begin: {begin}')
    client = httpx.AsyncClient()
    await parse_list(begin, client)


async def repair():
    all_news = await ChargingAllianceNews.all().order_by('-year', '-month')
    logger.info(f'repair data, total {len(all_news)}')
    expected_fields = [
        "total_charging_facilities",
        "public_charging_facilities",
        "private_charging_facilities",
        "public_rated_total_power",
        "public_average_power",
        "private_declared_capacity",
        "total_charging_capacity",
        "increase_charging_facilities",
        "increase_public_facilities",
        "increase_private_facilities",
        "year_NEV_sales",
    ]
    for index, news in enumerate(all_news):
        # try:
        #     parse_json = call_hunyuan(re.sub(r'\s+', '', news.origin_text), LLM_PROMPT, expected_fields)
        # except:
        #     continue
        # logger.info(f'parse json: {parse_json}')
        last_news = all_news[index + 1] if index < len(all_news) - 1 else None
        logger.info(
            f'reparing: {news.year}-{news.month}, last: {last_news.year}-{last_news.month} news_year_NEV_sales: {news.year_NEV_sales}, last_news_year_NEV_sales: {last_news.year_NEV_sales}'
        )
        if last_news and last_news.year_NEV_sales and news.year_NEV_sales:
            news.NEV_sales = news.year_NEV_sales - last_news.year_NEV_sales
        # for key, value in parse_json.items():
        #     org_value = getattr(news, key, None)
        #     if value is not None and value != org_value and hasattr(news, key):
        #         # å¦‚æœåŸæ•°æ®å·²ç»æœ‰å€¼ï¼Œä½ å¯ä»¥é€‰æ‹©è¦†ç›–æˆ–è€…ä¿ç•™ã€‚è¿™é‡Œé€‰æ‹©ã€å¼ºåˆ¶è¦†ç›–ã€‘ä»¥ä¿®å¤é”™è¯¯æ•°æ®
        #         setattr(news, key, value)
        await news.save()


if __name__ == '__main__':
    asyncio.run(main())
